# sample_etl

Общее описание системы и обработки данных с использованием Docker Compose, Spark, Hadoop и PostgreSQL

Docker Compose конфигурация:
    В файле docker-compose.yml определены сервисы, включая Apache Spark, Hadoop, PostgreSQL и API сервис.
    Каждый сервис настроен на своем соответствующем порту внутри среды Docker, а также имеет свое уникальное имя, позволяющее другим 
	контейнерам обращаться к нему.

API сервис:
    Flask API сервис запущен на отдельном контейнере с Python Slim образом.
    API предоставляет эндпоинты для добавления событий и запросов аналитических данных.

Добавление событий:
    При отправке POST-запроса на /event, данные события принимаются API сервисом.
    Затем данные передаются в PySpark, где создается DataFrame из JSON-данных события.
    DataFrame сохраняется в таблицу "events" в витрину на PostgreSQL через Spark JDBC connector.

Запрос аналитических данных:
    При отправке GET-запроса на /analytics/query, API сервис принимает параметры запроса.
    PySpark обрабатывает запрос, выполняя соответствующие операции с данными в Hadoop и Spark, используя таблицы и временные представления.
    Результаты анализа могут быть сохранены обратно в PostgreSQL или возвращены напрямую через API.

Хранение данных:
    Данные хранятся в Hadoop Distributed File System (HDFS) с доступом к ним через Hadoop Namenode и Datanode контейнеры.
    Результаты обработки и анализа данных сохраняются в PostgreSQL в виде таблиц или представлени.
